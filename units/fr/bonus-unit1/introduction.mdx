# Introduction

![Bonus Unit 1 Thumbnail](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit1/thumbnail.jpg)

Bienvenue dans cette premi√®re **Unit√© Bonus**, o√π vous apprendrez √† **ajuster finement un Grand Mod√®le de Langage (LLM) pour l'appel de fonctions** (*function calling*).

En termes de LLMs, l'appel de fonctions devient rapidement une technique *incontournable*. 

L'id√©e est que, plut√¥t que de s'appuyer uniquement sur des approches bas√©es sur des prompts comme nous l'avons fait dans l'Unit√© 1, l'appel de fonctions entra√Æne votre mod√®le √† **prendre des actions et interpr√©ter des observations pendant la phase d'entra√Ænement**, rendant votre IA plus robuste.

> **Quand dois-je faire cette Unit√© Bonus ?**
>
> Cette section est **optionnelle** et plus avanc√©e que l'Unit√© 1, donc n'h√©sitez pas √† faire cette unit√© maintenant ou √† la revisiter quand vos connaissances se seront am√©lior√©es gr√¢ce √† ce cours. 
>  
> Mais ne vous inqui√©tez pas, cette Unit√© Bonus est con√ßue pour avoir toutes les informations dont vous avez besoin, donc nous vous guiderons √† travers chaque concept central de l'ajustement fin d'un mod√®le pour l'appel de fonctions m√™me si vous n'avez pas encore appris le fonctionnement interne de l'ajustement fin.

La meilleure fa√ßon pour vous de pouvoir suivre cette Unit√© Bonus est de :

1. Savoir comment Ajuster finement un LLM avec *Transformers*, si ce n'est pas le cas [consultez ceci](https://huggingface.co/learn/nlp-course/chapter3/1?fw=pt).

2. Savoir comment utiliser `SFTTrainer` pour ajuster finement notre mod√®le, pour en savoir plus √† ce sujet [consultez cette documentation](https://huggingface.co/learn/nlp-course/en/chapter11/1). 

---

## Ce que vous apprendrez

1. **L'Appel de Fonctions** (*Function Calling*)  
   Comment les LLMs modernes structurent leurs conversations de mani√®re efficace leur permettant de d√©clencher des **Outils**.

2. **LoRA** (*Low-Rank Adaptation*)  
   Une m√©thode d'ajustement fin **l√©g√®re et efficace** qui r√©duit les co√ªts computationnels et de stockage. LoRA rend l'entra√Ænement de gros mod√®les *plus rapide, moins cher et plus facile* √† d√©ployer.

3. **Le Cycle R√©flexion ‚Üí Action ‚Üí Observation** dans les mod√®les d'appel de fonctions  
   Une approche simple mais puissante pour structurer comment votre mod√®le d√©cide quand (et comment) appeler des fonctions, suivre les √©tapes interm√©diaires et interpr√©ter les r√©sultats des Outils ou APIs externes.

4. **Nouveaux Tokens Sp√©ciaux**  
   Nous introduirons des **marqueurs sp√©ciaux** qui aident le mod√®le √† distinguer entre :
   - Le raisonnement interne "*chain-of-thought*"  
   - Les appels de fonctions sortants  
   - Les r√©ponses provenant d'outils externes

---

√Ä la fin de cette unit√© bonus, vous serez capable de :

- **Comprendre** le fonctionnement interne des APIs quand il s'agit d'Outils.  
- **Ajuster finement** un mod√®le en utilisant la technique LoRA.  
- **Impl√©menter** et **modifier** le cycle R√©flexion ‚Üí Action ‚Üí Observation pour cr√©er des flux de travail d'appel de fonctions robustes et maintenables.  
- **Concevoir et utiliser** des tokens sp√©ciaux pour s√©parer de mani√®re transparente le raisonnement interne du mod√®le de ses actions externes.

Et vous **aurez ajust√© finement votre propre mod√®le pour faire de l'appel de fonctions.** üî•

Plongeons dans **l'appel de fonctions** !