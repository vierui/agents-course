# Observabilit√© et √âvaluation des Agents IA

## üîé Qu'est-ce que l'Observabilit√© ?

L'observabilit√© consiste √† comprendre ce qui se passe √† l'int√©rieur de votre agent IA en regardant des signaux externes comme les logs, les m√©triques et les traces. Pour les agents IA, cela signifie suivre les actions, l'utilisation des outils, les appels de mod√®les et les r√©ponses pour d√©boguer et am√©liorer les performances de l'agent.

![Observability dashboard](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/langfuse-dashboard.png)

## üî≠ Pourquoi l'Observabilit√© des Agents est importante

Sans observabilit√©, les agents IA sont des "bo√Ætes noires". Les outils d'observabilit√© rendent les agents transparents, vous permettant de :

- Comprendre les compromis entre co√ªts et pr√©cision
- Mesurer la latence
- D√©tecter le langage nuisible et l'injection de prompts
- Surveiller les commentaires utilisateur

En d'autres termes, cela rend votre agent de d√©monstration pr√™t pour la production !

## üî® Outils d'Observabilit√©

Les outils d'observabilit√© courants pour les agents IA incluent des plateformes comme [Langfuse](https://langfuse.com) et [Arize](https://www.arize.com). Ces outils aident √† collecter des traces d√©taill√©es et offrent des tableaux de bord pour surveiller les m√©triques en temps r√©el, facilitant la d√©tection de probl√®mes et l'optimisation des performances.

Les outils d'observabilit√© varient largement dans leurs fonctionnalit√©s et capacit√©s. Certains outils sont *open source*, b√©n√©ficiant de grandes communaut√©s qui fa√ßonnent leurs feuilles de route et de nombreuses int√©grations. De plus, certains outils se sp√©cialisent dans des aspects sp√©cifiques des *LLMOps*‚Äîcomme l'observabilit√©, les √©valuations ou la gestion des prompts‚Äîtandis que d'autres sont con√ßus pour couvrir l'ensemble du flux de travail *LLMOps*. Nous vous encourageons √† explorer la documentation de diff√©rentes options pour choisir une solution qui fonctionne bien pour vous.

De nombreux frameworks d'agents tels que [smolagents](https://huggingface.co/docs/smolagents/v1.12.0/en/index) utilisent le standard [OpenTelemetry](https://opentelemetry.io/docs/) pour exposer les m√©tadonn√©es aux outils d'observabilit√©. En plus de cela, les outils d'observabilit√© construisent des instrumentations personnalis√©es pour permettre plus de flexibilit√© dans le monde en mouvement rapide des LLMs. Vous devriez consulter la documentation de l'outil que vous utilisez pour voir ce qui est pris en charge.

## üî¨Traces et Spans

Les outils d'observabilit√© repr√©sentent g√©n√©ralement les ex√©cutions d'agents comme des traces et des spans.

- **Les traces** repr√©sentent une t√¢che d'agent compl√®te du d√©but √† la fin (comme traiter une requ√™te utilisateur).
- **Les spans** sont des √©tapes individuelles dans la trace (comme appeler un mod√®le de langage ou r√©cup√©rer des donn√©es).

![Example of a smolagent trace in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/trace-tree.png)

## üìä M√©triques cl√©s √† surveiller

Voici quelques-unes des m√©triques les plus courantes que les outils d'observabilit√© surveillent :

**Latence :** √Ä quelle vitesse l'agent r√©pond-il ? Les longs temps d'attente impactent n√©gativement l'exp√©rience utilisateur. Vous devriez mesurer la latence pour les t√¢ches et les √©tapes individuelles en tra√ßant les ex√©cutions d'agent. Par exemple, un agent qui prend 20 secondes pour tous les appels de mod√®le pourrait √™tre acc√©l√©r√© en utilisant un mod√®le plus rapide ou en ex√©cutant les appels de mod√®le en parall√®le.

**Co√ªts :** Quelle est la d√©pense par ex√©cution d'agent ? Les agents IA s'appuient sur des appels LLM factur√©s par token ou des APIs externes. L'utilisation fr√©quente d'outils ou de multiples prompts peut rapidement augmenter les co√ªts. Par exemple, si un agent appelle un LLM cinq fois pour une am√©lioration de qualit√© marginale, vous devez √©valuer si le co√ªt est justifi√© ou si vous pourriez r√©duire le nombre d'appels ou utiliser un mod√®le moins cher. La surveillance en temps r√©el peut aussi aider √† identifier des pics inattendus (par exemple, des bugs causant des boucles d'API excessives).

**Erreurs de requ√™te :** Combien de requ√™tes l'agent a-t-il √©chou√©es ? Cela peut inclure des erreurs d'API ou des appels d'outils √©chou√©s. Pour rendre votre agent plus robuste contre ceux-ci en production, vous pouvez ensuite mettre en place des solutions de secours ou des tentatives. Par exemple, si le fournisseur LLM A est en panne, vous basculez vers le fournisseur LLM B comme sauvegarde.

**Commentaires utilisateur :** Impl√©menter des √©valuations utilisateur directes fournit des insights pr√©cieux. Cela peut inclure des √©valuations explicites (üëçpouces lev√©s/üëébaiss√©s, ‚≠ê1-5 √©toiles) ou des commentaires textuels. Des commentaires n√©gatifs constants devraient vous alerter car c'est un signe que l'agent ne fonctionne pas comme pr√©vu. 

**Commentaires utilisateur implicites :** Les comportements utilisateur fournissent des commentaires indirects m√™me sans √©valuations explicites. Cela peut inclure la reformulation imm√©diate de questions, des requ√™tes r√©p√©t√©es ou cliquer sur un bouton de r√©essai. Par exemple, si vous voyez que les utilisateurs posent r√©p√©titivement la m√™me question, c'est un signe que l'agent ne fonctionne pas comme pr√©vu.

**Pr√©cision :** √Ä quelle fr√©quence l'agent produit-il des sorties correctes ou souhaitables ? Les d√©finitions de pr√©cision varient (par exemple, exactitude de r√©solution de probl√®mes, pr√©cision de r√©cup√©ration d'informations, satisfaction utilisateur). La premi√®re √©tape est de d√©finir √† quoi ressemble le succ√®s pour votre agent. Vous pouvez suivre la pr√©cision via des v√©rifications automatis√©es, des scores d'√©valuation ou des labels de r√©ussite de t√¢ches. Par exemple, marquer les traces comme "r√©ussies" ou "√©chou√©es". 

**M√©triques d'√©valuation automatis√©e :** Vous pouvez aussi mettre en place des √©valuations automatis√©es. Par exemple, vous pouvez utiliser un LLM pour noter la sortie de l'agent, par exemple si elle est utile, pr√©cise ou non. Il existe aussi plusieurs biblioth√®ques *open source* qui vous aident √† noter diff√©rents aspects de l'agent. Par exemple [RAGAS](https://docs.ragas.io/) pour les agents RAG ou [LLM Guard](https://llm-guard.com/) pour d√©tecter le langage nuisible ou l'injection de prompts. 

En pratique, une combinaison de ces m√©triques donne la meilleure couverture de la sant√© d'un agent IA. Dans le [notebook d'exemple](https://colab.research.google.com/#fileId=https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit2/monitoring-and-evaluating-agents.ipynb) de ce chapitre, nous vous montrerons √† quoi ressemblent ces m√©triques dans des exemples r√©els mais d'abord, nous apprendrons √† quoi ressemble un flux de travail d'√©valuation typique.

## üëç √âvaluation des Agents IA

L'observabilit√© nous donne des m√©triques, mais l'√©valuation est le processus d'analyser ces donn√©es (et d'effectuer des tests) pour d√©terminer √† quel point un agent IA performe et comment il peut √™tre am√©lior√©. En d'autres termes, une fois que vous avez ces traces et m√©triques, comment les utilisez-vous pour juger l'agent et prendre des d√©cisions ? 

L'√©valuation r√©guli√®re est importante car les agents IA sont souvent non-d√©terministes et peuvent √©voluer (par des mises √† jour ou un comportement de mod√®le d√©rivant) ‚Äì sans √©valuation, vous ne sauriez pas si votre "agent intelligent" fait r√©ellement bien son travail ou s'il a r√©gress√©.

Il y a deux cat√©gories d'√©valuations pour les agents IA : **l'√©valuation en ligne** et **l'√©valuation hors ligne**. Les deux sont pr√©cieuses, et elles se compl√®tent. Nous commen√ßons g√©n√©ralement par l'√©valuation hors ligne, car c'est l'√©tape minimum n√©cessaire avant de d√©ployer tout agent.

### ü•∑ √âvaluation hors ligne

![Dataset items in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/example-dataset.png)

Cela implique d'√©valuer l'agent dans un environnement contr√¥l√©, typiquement en utilisant des jeux de donn√©es de test, pas des requ√™tes utilisateur en direct. Vous utilisez des jeux de donn√©es organis√©s o√π vous savez quelle est la sortie attendue ou le comportement correct, puis vous ex√©cutez votre agent sur ceux-ci. 

Par exemple, si vous avez construit un agent de probl√®mes de mots math√©matiques, vous pourriez avoir un [jeu de donn√©es de test](https://huggingface.co/datasets/gsm8k) de 100 probl√®mes avec des r√©ponses connues. L'√©valuation hors ligne est souvent effectu√©e pendant le d√©veloppement (et peut faire partie des pipelines CI/CD) pour v√©rifier les am√©liorations ou se prot√©ger contre les r√©gressions. L'avantage est que c'est **r√©p√©table et vous pouvez obtenir des m√©triques de pr√©cision claires puisque vous avez la v√©rit√© terrain**. Vous pourriez aussi simuler des requ√™tes utilisateur et mesurer les r√©ponses de l'agent contre des r√©ponses id√©ales ou utiliser des m√©triques automatis√©es comme d√©crit ci-dessus. 

Le d√©fi cl√© avec l'√©valuation hors ligne est d'assurer que votre jeu de donn√©es de test est complet et reste pertinent ‚Äì l'agent pourrait bien performer sur un jeu de test fixe mais rencontrer des requ√™tes tr√®s diff√©rentes en production. Par cons√©quent, vous devriez garder les jeux de test mis √† jour avec de nouveaux cas limites et des exemples qui refl√®tent des sc√©narios du monde r√©el. Un m√©lange de petits cas de "*smoke test*" et de plus grands ensembles d'√©valuation est utile : les petits ensembles pour des v√©rifications rapides et les plus grands pour des m√©triques de performance plus larges.

### üîÑ √âvaluation en ligne 

Cela se r√©f√®re √† l'√©valuation de l'agent dans un environnement en direct, du monde r√©el, c'est-√†-dire pendant l'utilisation r√©elle en production. L'√©valuation en ligne implique de surveiller les performances de l'agent sur de vraies interactions utilisateur et d'analyser les r√©sultats en continu. 

Par exemple, vous pourriez suivre les taux de succ√®s, les scores de satisfaction utilisateur, ou d'autres m√©triques sur le trafic en direct. L'avantage de l'√©valuation en ligne est qu'elle **capture des choses que vous pourriez ne pas anticiper dans un environnement de laboratoire** ‚Äì vous pouvez observer la d√©rive du mod√®le au fil du temps (si l'efficacit√© de l'agent se d√©grade quand les patterns d'entr√©e changent) et attraper des requ√™tes ou situations inattendues qui n'√©taient pas dans vos donn√©es de test. Elle fournit une vraie image de comment l'agent se comporte dans la nature. 

L'√©valuation en ligne implique souvent de collecter des commentaires utilisateur implicites et explicites, comme discut√©, et possiblement d'ex√©cuter des tests d'ombre ou des tests A/B (o√π une nouvelle version de l'agent fonctionne en parall√®le pour comparer √† l'ancienne). Le d√©fi est qu'il peut √™tre d√©licat d'obtenir des labels ou scores fiables pour les interactions en direct ‚Äì vous pourriez vous appuyer sur les commentaires utilisateur ou des m√©triques en aval (comme est-ce que l'utilisateur a cliqu√© sur le r√©sultat). 

### ü§ù Combiner les deux

En pratique, l'√©valuation r√©ussie d'agent IA m√©lange les m√©thodes **en ligne** et **hors ligne**. Vous pourriez ex√©cuter des benchmarks hors ligne r√©guliers pour noter quantitativement votre agent sur des t√¢ches d√©finies et surveiller continuellement l'utilisation en direct pour attraper des choses que les benchmarks ratent. Par exemple, les tests hors ligne peuvent attraper si le taux de succ√®s d'un agent de g√©n√©ration de code sur un ensemble connu de probl√®mes s'am√©liore, tandis que la surveillance en ligne pourrait vous alerter que les utilisateurs ont commenc√© √† poser une nouvelle cat√©gorie de question avec laquelle l'agent lutte. Combiner les deux donne une image plus robuste. 

En fait, de nombreuses √©quipes adoptent une boucle : _√©valuation hors ligne ‚Üí d√©ployer nouvelle version d'agent ‚Üí surveiller les m√©triques en ligne et collecter de nouveaux exemples d'√©chec ‚Üí ajouter ces exemples au jeu de test hors ligne ‚Üí it√©rer_. De cette fa√ßon, l'√©valuation est continue et s'am√©liore constamment.

## üßë‚Äçüíª Voyons comment cela fonctionne en pratique

Dans la prochaine section, nous verrons des exemples de comment nous pouvons utiliser des outils d'observabilit√© pour surveiller et √©valuer notre agent.