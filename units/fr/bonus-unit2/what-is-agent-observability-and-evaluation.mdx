# Qu'est-ce que l'observabilitÃ© et l'Ã©valuation des agents ?

## ğŸ” Qu'est-ce que l'ObservabilitÃ© ?

L'observabilitÃ© consiste Ã  comprendre ce qui se passe Ã  l'intÃ©rieur de votre agent en regardant des signaux externes comme les logs, les mÃ©triques et les traces. Pour les agents, cela signifie suivre les actions, l'utilisation des outils, les appels de modÃ¨les et les rÃ©ponses pour dÃ©boguer et amÃ©liorer les performances de l'agent.

![Observability dashboard](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/langfuse-dashboard.png)

## ğŸ”­ Pourquoi l'observabilitÃ© des agents est importante

Sans observabilitÃ©, les agents sont des Â« boÃ®tes noires Â». Les outils d'observabilitÃ© rendent les agents transparents, vous permettant de :

- Comprendre les compromis entre coÃ»ts et prÃ©cision
- Mesurer la latence
- DÃ©tecter le langage nuisible et l'injection de *prompts*
- Surveiller les retours utilisateur

En d'autres termes, cela permet de passer votre agent de dÃ©monstration Ã  de la production !

## ğŸ”¨ Outils d'observabilitÃ©

Les outils d'observabilitÃ© courants pour les agents incluent des plateformes comme [Langfuse](https://langfuse.com) et [Arize](https://www.arize.com). Ces outils aident Ã  collecter des traces dÃ©taillÃ©es et offrent des tableaux de bord pour surveiller les mÃ©triques en temps rÃ©el, facilitant la dÃ©tection de problÃ¨mes et l'optimisation des performances.

Les outils d'observabilitÃ© varient largement dans leurs fonctionnalitÃ©s et capacitÃ©s. Certains outils sont *open source*, bÃ©nÃ©ficiant de grandes communautÃ©s qui faÃ§onnent leurs feuilles de route et de nombreuses intÃ©grations. De plus, certains outils se spÃ©cialisent dans des aspects spÃ©cifiques des LLMOps (comme l'observabilitÃ©, les Ã©valuations ou la gestion des prompts) tandis que d'autres sont conÃ§us pour couvrir l'ensemble du *workflow* LLMOps. Nous vous encourageons Ã  explorer la documentation de diffÃ©rentes options pour choisir la solution qui fonctionne bien pour vous.

De nombreux *frameworks* d'agents tels que [smolagents](https://huggingface.co/docs/smolagents/v1.12.0/en/index) utilisent le standard [OpenTelemetry](https://opentelemetry.io/docs/) pour exposer les mÃ©tadonnÃ©es aux outils d'observabilitÃ©. En plus de cela, les outils d'observabilitÃ© construisent des instrumentations personnalisÃ©es pour permettre plus de flexibilitÃ© dans le monde en mouvement rapide des LLM. Vous devriez consulter la documentation de l'outil que vous utilisez pour voir ce qui est pris en charge.

## ğŸ”¬Traces et *Spans*

Les outils d'observabilitÃ© reprÃ©sentent gÃ©nÃ©ralement les exÃ©cutions d'agents comme des traces et des *spans*.

- **Les traces** reprÃ©sentent une tÃ¢che d'agent complÃ¨te du dÃ©but Ã  la fin (comme traiter une requÃªte utilisateur).
- **Les spans** sont des Ã©tapes individuelles dans la trace (comme appeler un modÃ¨le de langage ou rÃ©cupÃ©rer des donnÃ©es).

![Example of a smolagent trace in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/trace-tree.png)

## ğŸ“Š MÃ©triques clÃ©s Ã  surveiller

Voici quelques-unes des mÃ©triques les plus courantes que les outils d'observabilitÃ© surveillent :

**Latence :** Ã€ quelle vitesse l'agent rÃ©pond-il ? Les longs temps d'attente impactent nÃ©gativement l'expÃ©rience utilisateur. Vous devriez mesurer la latence pour les tÃ¢ches et les Ã©tapes individuelles en traÃ§ant les exÃ©cutions d'agent. Par exemple, un agent qui prend 20 secondes pour tous les appels de modÃ¨le pourrait Ãªtre accÃ©lÃ©rÃ© en utilisant un modÃ¨le plus rapide ou en exÃ©cutant les appels de modÃ¨le en parallÃ¨le.

**CoÃ»ts :** Quelle est la dÃ©pense par exÃ©cution d'agent ? Les agents IA s'appuient sur des LLM facturÃ©s par *token* ou des API externes. L'utilisation frÃ©quente d'outils ou de beaucoup de *prompts* peut rapidement augmenter les coÃ»ts. Par exemple, si un agent appelle un *LLM* cinq fois pour une amÃ©lioration de qualitÃ© marginale, vous devez Ã©valuer si le coÃ»t est justifiÃ© ou si vous pourriez rÃ©duire le nombre d'appels ou utiliser un modÃ¨le moins cher. La surveillance en temps rÃ©el peut aussi aider Ã  identifier des pics inattendus (par exemple, des bugs causant des boucles d'API excessives).

**Erreurs de requÃªte :** Combien de requÃªtes l'agent a-t-il Ã©chouÃ©es ? Cela peut inclure des erreurs d'API ou des appels d'outils Ã©chouÃ©s. Pour rendre votre agent plus robuste contre ceux-ci en production, vous pouvez ensuite mettre en place des solutions de secours ou des tentatives. Par exemple, si le fournisseur de LLM A est en panne, vous basculez vers le fournisseur de LLM B comme *backup*.

**Retours utilisateur :** ImplÃ©menter des Ã©valuations utilisateur directes fournit des informations prÃ©cieuses. Cela peut inclure des Ã©valuations explicites (pouces levÃ©s ğŸ‘/baissÃ©s ğŸ‘, â­1-5 Ã©toiles) ou des commentaires textuels. Des commentaires nÃ©gatifs devraient vous alerter car c'est un signe que l'agent ne fonctionne pas comme prÃ©vu. 

**Retours utilisateur implicites :** Les comportements utilisateur fournissent des commentaires indirects mÃªme sans Ã©valuations explicites. Cela peut inclure la reformulation immÃ©diate de questions, des requÃªtes rÃ©pÃ©tÃ©es ou cliquer sur un bouton de rÃ©essai. Par exemple, si vous voyez que les utilisateurs posent rÃ©pÃ©titivement la mÃªme question, c'est un signe que l'agent ne fonctionne pas comme prÃ©vu.

**PrÃ©cision :** Ã€ quelle frÃ©quence l'agent produit-il des sorties correctes ou souhaitables ? Les dÃ©finitions de prÃ©cision varient (par exemple, exactitude de rÃ©solution de problÃ¨mes, prÃ©cision de rÃ©cupÃ©ration d'informations, satisfaction utilisateur). La premiÃ¨re Ã©tape est de dÃ©finir Ã  quoi ressemble le succÃ¨s pour votre agent. Vous pouvez suivre la prÃ©cision via des vÃ©rifications automatisÃ©es, des scores d'Ã©valuation ou des labels de rÃ©ussite de tÃ¢ches. Par exemple, marquer les traces comme Â« rÃ©ussies Â» ou Â« Ã©chouÃ©es Â». 

**MÃ©triques d'Ã©valuation automatisÃ©e :** Vous pouvez aussi mettre en place des Ã©valuations automatisÃ©es. Par exemple, vous pouvez utiliser un LLM pour noter la sortie de l'agent, par exemple si elle est utile, prÃ©cise ou non. Il existe aussi plusieurs bibliothÃ¨ques *open source* qui vous aident Ã  noter diffÃ©rents aspects de l'agent. Par exemple [RAGAS](https://docs.ragas.io/) pour les agents RAG ou [LLM Guard](https://llm-guard.com/) pour dÃ©tecter le langage nuisible ou l'injection de *prompts*. 

En pratique, une combinaison de ces mÃ©triques donne la meilleure couverture de la santÃ© d'un agent IA. Dans le [*notebook* d'exemple](https://colab.research.google.com/#fileId=https://huggingface.co/agents-course/notebooks/blob/main/fr/bonus-unit2/monitoring-and-evaluating-agents.ipynb) de ce chapitre, nous vous montrerons Ã  quoi ressemblent ces mÃ©triques dans des exemples rÃ©els mais d'abord, nous apprendrons Ã  quoi ressemble un *workflow* d'Ã©valuation typique.

## ğŸ‘ Ã‰valuation des agents

L'observabilitÃ© nous donne des mÃ©triques, mais l'Ã©valuation est le processus d'analyser ces donnÃ©es (et d'effectuer des tests) pour dÃ©terminer Ã  quel point un agent performe et comment il peut Ãªtre amÃ©liorÃ©. En d'autres termes, une fois que vous avez ces traces et mÃ©triques, comment les utilisez-vous pour juger l'agent et prendre des dÃ©cisions ? 

L'Ã©valuation standard est importante car les agents sont souvent non-dÃ©terministes et peuvent Ã©voluer (par des mises Ã  jour ou un comportement de modÃ¨le dÃ©rivant) â€“ sans Ã©valuation, vous ne sauriez pas si votre agent fait rÃ©ellement bien son travail ou s'il a rÃ©gressÃ©.

Il y a deux catÃ©gories d'Ã©valuations pour les agents : **l'Ã©valuation en ligne** et **l'Ã©valuation hors ligne**. Les deux sont prÃ©cieuses, et elles se complÃ¨tent. Nous commenÃ§ons gÃ©nÃ©ralement par l'Ã©valuation hors ligne, car c'est l'Ã©tape minimum nÃ©cessaire avant de dÃ©ployer tout agent.

### ğŸ¥· Ã‰valuation hors ligne

![Dataset items in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/example-dataset.png)

Cela implique d'Ã©valuer l'agent dans un environnement contrÃ´lÃ©, typiquement en utilisant des jeux de donnÃ©es de test, pas des requÃªtes utilisateur en direct. Vous utilisez des jeux de donnÃ©es organisÃ©s oÃ¹ vous savez quelle est la sortie attendue ou le comportement correct, puis vous exÃ©cutez votre agent sur ceux-ci. 

Par exemple, si vous avez construit un agent rÃ©solvant des problÃ¨mes mathÃ©matiques, vous pourriez avoir un [jeu de donnÃ©es de test](https://huggingface.co/datasets/gsm8k) de 100 problÃ¨mes avec des rÃ©ponses connues. L'Ã©valuation hors ligne est souvent effectuÃ©e pendant le dÃ©veloppement (et peut faire partie des pipelines CI/CD) pour vÃ©rifier les amÃ©liorations ou se protÃ©ger contre les rÃ©gressions. L'avantage est que c'est **rÃ©pÃ©table et vous pouvez obtenir des mÃ©triques de prÃ©cision claires puisque vous avez la vÃ©ritÃ© terrain**. Vous pourriez aussi simuler des requÃªtes utilisateur et mesurer les rÃ©ponses de l'agent contre des rÃ©ponses idÃ©ales ou utiliser des mÃ©triques automatisÃ©es comme dÃ©crit ci-dessus. 

Le dÃ©fi clÃ© avec l'Ã©valuation hors ligne est d'assurer que votre jeu de donnÃ©es de test est complet et reste pertinent : l'agent pourrait bien performer sur un jeu de test fixe mais rencontrer des requÃªtes trÃ¨s diffÃ©rentes en production. Par consÃ©quent, vous devriez garder les jeux de test mis Ã  jour avec de nouveaux cas limites et des exemples qui reflÃ¨tent des scÃ©narios du monde rÃ©el. Un mÃ©lange de petits cas de *smoke test* et de plus grands jeux d'Ã©valuation est utile : les petits jeux pour des vÃ©rifications rapides et les plus grands pour des mÃ©triques de performance plus larges.

### ğŸ”„ Ã‰valuation en ligne 

Cela se rÃ©fÃ¨re Ã  l'Ã©valuation de l'agent dans un environnement en direct, du monde rÃ©el, c'est-Ã -dire pendant l'utilisation rÃ©elle en production. L'Ã©valuation en ligne implique de surveiller les performances de l'agent sur de vraies interactions utilisateur et d'analyser les rÃ©sultats en continu. 

Par exemple, vous pourriez suivre les taux de succÃ¨s, les scores de satisfaction utilisateur, ou d'autres mÃ©triques sur le trafic en direct. L'avantage de l'Ã©valuation en ligne est qu'elle **capture des choses que vous pourriez ne pas anticiper dans un environnement de laboratoire**. Vous pouvez observer la dÃ©rive du modÃ¨le au fil du temps (si l'efficacitÃ© de l'agent se dÃ©grade quand les entrÃ©es changent) et attraper des requÃªtes ou situations inattendues qui n'Ã©taient pas dans vos donnÃ©es de test. Elle fournit une vraie image de comment l'agent se comporte dans la nature. 

L'Ã©valuation en ligne implique souvent de collecter des retours utilisateur implicites et explicites, comme discutÃ©, et possiblement d'exÃ©cuter des *shadow tests* ou des tests A/B (oÃ¹ une nouvelle version de l'agent fonctionne en parallÃ¨le pour comparer Ã  l'ancienne). Le dÃ©fi est qu'il peut Ãªtre dÃ©licat d'obtenir des labels ou scores fiables pour les interactions en direct. Vous pourriez vous appuyer sur les retours utilisateur ou des mÃ©triques en aval (comme est-ce que l'utilisateur a cliquÃ© sur le rÃ©sultat). 

### ğŸ¤ Combiner les deux

En pratique, l'Ã©valuation rÃ©ussie d'agent mÃ©lange les mÃ©thodes **en ligne** et **hors ligne**. Vous pourriez exÃ©cuter des *benchmarks* hors ligne standards pour noter quantitativement votre agent sur des tÃ¢ches dÃ©finies et surveiller continuellement l'utilisation en direct pour attraper des choses que les *benchmarks* ratent. Par exemple, les tests hors ligne peuvent attraper si le taux de succÃ¨s d'un agent de gÃ©nÃ©ration de code sur un jeu connu de problÃ¨mes s'amÃ©liore, tandis que la surveillance en ligne pourrait vous alerter que les utilisateurs ont commencÃ© Ã  poser une nouvelle catÃ©gorie de question avec laquelle l'agent lutte. Combiner les deux donne une image plus robuste. 

En fait, de nombreuses Ã©quipes adoptent une boucle : _Ã©valuation hors ligne â†’ dÃ©ployer une nouvelle version d'agent â†’ surveiller les mÃ©triques en ligne et collecter de nouveaux exemples d'Ã©chec â†’ ajouter ces exemples au jeu de test hors ligne â†’ itÃ©rer_. De cette faÃ§on, l'Ã©valuation est continue et s'amÃ©liore constamment.

## ğŸ§‘â€ğŸ’» Voyons comment cela fonctionne en pratique

Dans la prochaine section, nous verrons des exemples de comment nous pouvons utiliser des outils d'observabilitÃ© pour surveiller et Ã©valuer notre agent.