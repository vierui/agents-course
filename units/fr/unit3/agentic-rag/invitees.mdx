# Cr√©ation d'un RAG pour converser avec les invit√©s

Alfred, votre agent de confiance, se pr√©pare pour le gala le plus extravagant du si√®cle. Pour s'assurer que l'√©v√©nement se d√©roule sans encombre, il a besoin d'un acc√®s rapide √† des informations √† jour sur chaque invit√©. Aidons le en cr√©ant un outil RAG aliment√© par notre jeu de donn√©es personnalis√©.

## Pourquoi un RAG pour un gala ?

Imaginez Alfred se m√™lant aux invit√©s, ayant besoin de se rappeler des d√©tails sp√©cifiques sur chaque personne √† tout moment. Un LLM traditionnel pourrait avoir du mal avec cette t√¢che parce que :

1. La liste des invit√©s est sp√©cifique √† votre √©v√©nement et ne fait pas partie des donn√©es d'entra√Ænement du mod√®le
2. Les informations sur les invit√©s peuvent changer ou √™tre mises √† jour fr√©quemment
3. Alfred doit r√©cup√©rer des d√©tails pr√©cis comme les adresses email

C'est l√† que le RAG brille ! En combinant un syst√®me de r√©cup√©ration avec un LLM, Alfred peut acc√©der √† des informations pr√©cises et √† jour sur vos invit√©s √† la demande.

> [!TIP]
> Vous pouvez choisir n'importe lequel des <i>frameworks</i> couverts dans le cours pour ce cas d'usage. S√©lectionnez votre option pr√©f√©r√©e dans les onglets de code.

## Configuration de notre application

Dans cette unit√©, nous d√©velopperons notre agent au sein d'un *Space*, sous la forme d'un projet Python structur√©. Cette approche nous aide √† maintenir un code propre et modulaire en organisant diff√©rentes fonctionnalit√©s dans des fichiers s√©par√©s. De plus, cela permet un cas d'usage plus r√©aliste o√π vous d√©ploieriez l'application pour une utilisation publique.

### Structure du projet

- **`tools.py`** ‚Äì Fournit des outils auxiliaires pour l'agent.
- **`retriever.py`** ‚Äì Impl√©mente les fonctions de r√©cup√©ration pour soutenir l'acc√®s √† la connaissance.
- **`app.py`** ‚Äì Int√®gre tous les composants dans un agent enti√®rement fonctionnel, que nous finaliserons dans la derni√®re partie de cette unit√©.

Pour une r√©f√©rence pratique, consultez [ce *Space*](https://huggingface.co/spaces/agents-course/Unit_3_Agentic_RAG) sur un RAG agentique. N'h√©sitez pas √† le cloner et √† exp√©rimenter !

Vous pouvez tester directement l'agent ci-dessous :

<iframe
	src="https://agents-course-unit-3-agentic-rag.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>

## Aper√ßu du jeu de donn√©es

Notre jeu de donn√©es [`agents-course/unit3-invitees`](https://huggingface.co/datasets/agents-course/unit3-invitees/) contient les champs suivants pour chaque invit√© :

- **Name** : Nom complet de l'invit√©
- **Relation** : Comment l'invit√© est li√© √† l'h√¥te
- **Description** : Une br√®ve biographie ou des faits int√©ressants sur l'invit√©
- **Email Address** : Informations de contact pour envoyer des invitations ou des suivis

Voici un aper√ßu du jeu de donn√©es :
<iframe
  src="https://huggingface.co/datasets/agents-course/unit3-invitees/embed/viewer/default/train"
  frameborder="0"
  width="100%"
  height="560px"
></iframe>

> [!TIP]
> Dans un sc√©nario r√©el, ce jeu de donn√©es pourrait √™tre √©tendu pour inclure les pr√©f√©rences alimentaires, les int√©r√™ts pour les cadeaux, les sujets de conversation √† √©viter, et d'autres d√©tails utiles pour un h√¥te.

## Construction d'un outil pour un livre d'or

Nous allons cr√©er un outil personnalis√© qu'Alfred peut utiliser pour r√©cup√©rer rapidement les informations sur les invit√©s pendant le gala. D√©composons cela en trois √©tapes g√©rables :

1. Charger et pr√©parer le jeu de donn√©es
2. Cr√©er l'outil de r√©cup√©ration
3. Int√©grer l'outil √† Alfred

Commen√ßons par charger et pr√©parer le jeu de donn√©es !

### √âtape 1 : Charger et pr√©parer le jeu de donn√©es

Tout d'abord, nous devons transformer nos donn√©es brutes sur les invit√©s en un format optimis√© pour la r√©cup√©ration.

<hfoptions id="agents-frameworks">
<hfoption id="smolagents">

Nous utiliserons la biblioth√®que `datasets` d'Hugging Face pour charger le jeu de donn√©es et le convertir en une liste d'objets `Document` du module `langchain.docstore.document`.

```python
import datasets
from langchain_core.documents import Document

# Charger le jeu de donn√©es
guest_dataset = datasets.load_dataset("agents-course/unit3-invitees", split="train")

# Convertir les entr√©es du jeu de donn√©es en objets Document
docs = [
    Document(
        page_content="\n".join([
            f"Name: {guest['name']}",
            f"Relation: {guest['relation']}",
            f"Description: {guest['description']}",
            f"Email: {guest['email']}"
        ]),
        metadata={"name": guest["name"]}
    )
    for guest in guest_dataset
]

```

</hfoption>
<hfoption id="llama-index">

Nous utiliserons la biblioth√®que `datasets` d'Hugging Face pour charger le jeu de donn√©es et le convertir en une liste d'objets `Document` du module `llama_index.core.schema`.

```python
import datasets
from llama_index.core.schema import Document

# Charger le jeu de donn√©es
guest_dataset = datasets.load_dataset("agents-course/unit3-invitees", split="train")

# Convertir les entr√©es du jeu de donn√©es en objets Document
docs = [
    Document(
        text="\n".join([
            f"Name: {guest_dataset['name'][i]}",
            f"Relation: {guest_dataset['relation'][i]}",
            f"Description: {guest_dataset['description'][i]}",
            f"Email: {guest_dataset['email'][i]}"
        ]),
        metadata={"name": guest_dataset['name'][i]}
    )
    for i in range(len(guest_dataset))
]
```

</hfoption>
<hfoption id="langgraph">

Nous utiliserons la biblioth√®que `datasets` d'Hugging Face pour charger le jeu de donn√©es et le convertir en une liste d'objets `Document` du module `langchain.docstore.document`.

```python
import datasets
from langchain_core.documents import Document

# Charger le jeu de donn√©es
guest_dataset = datasets.load_dataset("agents-course/unit3-invitees", split="train")

# Convertir les entr√©es du jeu de donn√©es en objets Document
docs = [
    Document(
        page_content="\n".join([
            f"Name: {guest['name']}",
            f"Relation: {guest['relation']}",
            f"Description: {guest['description']}",
            f"Email: {guest['email']}"
        ]),
        metadata={"name": guest["name"]}
    )
    for guest in guest_dataset
]
```

</hfoption>
</hfoptions>

Dans le code ci-dessus, nous :
- Chargeons le jeu de donn√©es
- Convertissons chaque entr√©e d'invit√© en un objet `Document` avec du contenu format√©
- Stockons les objets `Document` dans une liste

Cela signifie que nous avons toutes nos donn√©es bien disponibles pour pouvoir commencer √† configurer notre r√©cup√©ration.

### √âtape 2 : Cr√©er l'outil de r√©cup√©ration

Maintenant, cr√©ons un outil personnalis√© qu'Alfred peut utiliser pour rechercher dans nos informations sur les invit√©s.

<hfoptions id="agents-frameworks">
<hfoption id="smolagents">

Nous utiliserons le `BM25Retriever` du module `langchain_community.retrievers` pour cr√©er un outil de r√©cup√©ration.

> [!TIP]
> Le <code>BM25Retriever</code> est un excellent point de d√©part pour la r√©cup√©ration, mais pour une recherche s√©mantique plus avanc√©e, vous pourriez consid√©rer l'utilisation de r√©cup√©rateurs bas√©s sur des <i>embeddings</i> comme ceux de <a href="https://www.sbert.net/">sentence-transformers</a>.

```python
from smolagents import Tool
from langchain_community.retrievers import BM25Retriever

class GuestInfoRetrieverTool(Tool):
    name = "guest_info_retriever"
    description = "R√©cup√®re des informations d√©taill√©es sur les invit√©s du gala bas√©es sur leur nom ou relation."
    inputs = {
        "query": {
            "type": "string",
            "description": "Le nom ou la relation de l'invit√© sur lequel vous voulez des informations."
        }
    }
    output_type = "string"

    def __init__(self, docs):
        self.is_initialized = False
        self.retriever = BM25Retriever.from_documents(docs)

    def forward(self, query: str):
        results = self.retriever.get_relevant_documents(query)
        if results:
            return "\n\n".join([doc.page_content for doc in results[:3]])
        else:
            return "Aucune information d'invit√© correspondante trouv√©e."

# Initialiser l'outil
guest_info_tool = GuestInfoRetrieverTool(docs)
```

Comprenons cet outil √©tape par √©tape :
- Le `name` et la `description` aident l'agent √† comprendre quand et comment utiliser cet outil
- Les `inputs` d√©finissent quels param√®tres l'outil attend (dans ce cas, une requ√™te de recherche)
- Nous utilisons un `BM25Retriever`, qui est un algorithme de r√©cup√©ration de texte puissant qui ne n√©cessite pas d'*embeddings*
- La m√©thode `forward` traite la requ√™te et retourne les informations d'invit√© les plus pertinentes

</hfoption>
<hfoption id="llama-index">

Nous utiliserons le `BM25Retriever` du module `llama_index.retrievers.bm25` pour cr√©er un outil de r√©cup√©ration.

> [!TIP]
> Le <code>BM25Retriever</code> est un excellent point de d√©part pour la r√©cup√©ration, mais pour une recherche s√©mantique plus avanc√©e, vous pourriez consid√©rer l'utilisation de r√©cup√©rateurs bas√©s sur des *embeddings* comme ceux de <a href="https://www.sbert.net/">sentence-transformers</a>.

```python
from llama_index.core.tools import FunctionTool
from llama_index.retrievers.bm25 import BM25Retriever

bm25_retriever = BM25Retriever.from_defaults(nodes=docs)

def get_guest_info_retriever(query: str) -> str:
    """R√©cup√®re des informations d√©taill√©es sur les invit√©s du gala bas√©es sur leur nom ou relation."""
    results = bm25_retriever.retrieve(query)
    if results:
        return "\n\n".join([doc.text for doc in results[:3]])
    else:
        return "Aucune information d'invit√© correspondante trouv√©e."

# Initialiser l'outil
guest_info_tool = FunctionTool.from_defaults(get_guest_info_retriever)
```

Comprenons cet outil √©tape par √©tape :
- La *docstring* aide l'agent √† comprendre quand et comment utiliser cet outil
- Les d√©corateurs de type d√©finissent quels param√®tres l'outil attend (dans ce cas, une requ√™te de recherche)
- Nous utilisons un `BM25Retriever`, qui est un algorithme de r√©cup√©ration de texte puissant qui ne n√©cessite pas d'*embeddings*
- La m√©thode traite la requ√™te et retourne les informations d'invit√© les plus pertinentes

</hfoption>
<hfoption id="langgraph">

Nous utiliserons le `BM25Retriever` du module `langchain_community.retrievers` pour cr√©er un outil de r√©cup√©ration.

> [!TIP]
> Le <code>BM25Retriever</code> est un excellent point de d√©part pour la r√©cup√©ration, mais pour une recherche s√©mantique plus avanc√©e, vous pourriez consid√©rer l'utilisation de r√©cup√©rateurs bas√©s sur des *embeddings* comme ceux de <a href="https://www.sbert.net/">sentence-transformers</a>.

```python
from langchain_community.retrievers import BM25Retriever
from langchain.tools import Tool

bm25_retriever = BM25Retriever.from_documents(docs)

def extract_text(query: str) -> str:
    """R√©cup√®re des informations d√©taill√©es sur les invit√©s du gala bas√©es sur leur nom ou relation."""
    results = bm25_retriever.invoke(query)
    if results:
        return "\n\n".join([doc.page_content for doc in results[:3]])
    else:
        return "Aucune information d'invit√© correspondante trouv√©e."

guest_info_tool = Tool(
    name="guest_info_retriever",
    func=extract_text,
    description="R√©cup√®re des informations d√©taill√©es sur les invit√©s du gala bas√©es sur leur nom ou relation."
)
```

Comprenons cet outil √©tape par √©tape :
- Le `name` et la `description` aident l'agent √† comprendre quand et comment utiliser cet outil
- Les d√©corateurs de type d√©finissent quels param√®tres l'outil attend (dans ce cas, une requ√™te de recherche)
- Nous utilisons un `BM25Retriever`, qui est un algorithme de r√©cup√©ration de texte puissant qui ne n√©cessite pas d'*embeddings*
- La m√©thode traite la requ√™te et retourne les informations d'invit√© les plus pertinentes

</hfoption>
</hfoptions>

### √âtape 3 : Int√©grer l'outil avec Alfred

Enfin, assemblons le tout en cr√©ant notre agent et en l'√©quipant de notre outil personnalis√© :

<hfoptions id="agents-frameworks">
<hfoption id="smolagents">

```python
from smolagents import CodeAgent, InferenceClientModel

# Initialiser le mod√®le Hugging Face
model = InferenceClientModel()

# Cr√©er Alfred, notre agent de gala, avec l'outil d'informations sur les invit√©s
alfred = CodeAgent(tools=[guest_info_tool], model=model)

# Exemple de requ√™te qu'Alfred pourrait recevoir pendant le gala
response = alfred.run("Parlez-moi de notre invit√©e nomm√©e 'Lady Ada Lovelace'.")

print("üé© R√©ponse d'Alfred :")
print(response)
```

Sortie attendue :

```
üé© R√©ponse d'Alfred :
Bas√© sur les informations que j'ai r√©cup√©r√©es, Lady Ada Lovelace est une math√©maticienne estim√©e et une amie. Elle est renomm√©e pour son travail pionnier en math√©matiques et en informatique, souvent c√©l√©br√©e comme la premi√®re programmeuse informatique en raison de son travail sur la machine analytique de Charles Babbage. Son adresse email est ada.lovelace@example.com.
```

Ce qui se passe dans cette √©tape finale :
- Nous initialisons un mod√®le Hugging Face en utilisant la classe `InferenceClientModel`
- Nous cr√©ons notre agent (Alfred) comme un `CodeAgent`, qui peut ex√©cuter du code Python pour r√©soudre des probl√®mes
- Nous demandons √† Alfred de r√©cup√©rer des informations sur une invit√©e nomm√©e "Lady Ada Lovelace"

</hfoption>
<hfoption id="llama-index">

```python
from llama_index.core.agent.workflow import AgentWorkflow
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI

# Initialiser le mod√®le Hugging Face
llm = HuggingFaceInferenceAPI(model_name="Qwen/Qwen2.5-Coder-32B-Instruct")

# Cr√©er Alfred, notre agent de gala, avec l'outil d'informations sur les invit√©s
alfred = AgentWorkflow.from_tools_or_functions(
    [guest_info_tool],
    llm=llm,
)

# Exemple de requ√™te qu'Alfred pourrait recevoir pendant le gala
response = await alfred.run("Parlez-moi de notre invit√©e nomm√©e 'Lady Ada Lovelace'.")

print("üé© R√©ponse d'Alfred :")
print(response)
```

Sortie attendue :

```
üé© R√©ponse d'Alfred :
Lady Ada Lovelace est une math√©maticienne estim√©e et amie, renomm√©e pour son travail pionnier en math√©matiques et en informatique. Elle est c√©l√©br√©e comme la premi√®re programmeuse informatique en raison de son travail sur la machine analytique de Charles Babbage. Son email est ada.lovelace@example.com.
```

Ce qui se passe dans cette √©tape finale :
- Nous initialisons un mod√®le Hugging Face en utilisant la classe `HuggingFaceInferenceAPI`
- Nous cr√©ons notre agent (Alfred) comme un `AgentWorkflow`, incluant l'outil que nous venons de cr√©er
- Nous demandons √† Alfred de r√©cup√©rer des informations sur une invit√©e nomm√©e "Lady Ada Lovelace"

</hfoption>
<hfoption id="langgraph">

```python
from typing import TypedDict, Annotated
from langgraph.graph.message import add_messages
from langchain_core.messages import AnyMessage, HumanMessage, AIMessage
from langgraph.prebuilt import ToolNode
from langgraph.graph import START, StateGraph
from langgraph.prebuilt import tools_condition
from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace

# G√©n√©rer l'interface de chat, incluant les outils
llm = HuggingFaceEndpoint(
    repo_id="Qwen/Qwen2.5-Coder-32B-Instruct",
    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,
)

chat = ChatHuggingFace(llm=llm, verbose=True)
tools = [guest_info_tool]
chat_with_tools = chat.bind_tools(tools)

# G√©n√©rer l'AgentState et le graphe d'agent
class AgentState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]

def assistant(state: AgentState):
    return {
        "messages": [chat_with_tools.invoke(state["messages"])],
    }

## Le graphe
builder = StateGraph(AgentState)

# D√©finir les n≈ìuds : ils font le travail
builder.add_node("assistant", assistant)
builder.add_node("tools", ToolNode(tools))

# D√©finir les ar√™tes : elles d√©terminent comment le flux de contr√¥le se d√©place
builder.add_edge(START, "assistant")
builder.add_conditional_edges(
    "assistant",
    # Si le dernier message n√©cessite un outil, router vers les outils
    # Sinon, fournir une r√©ponse directe
    tools_condition,
)
builder.add_edge("tools", "assistant")
alfred = builder.compile()

messages = [HumanMessage(content="Parlez-moi de notre invit√©e nomm√©e 'Lady Ada Lovelace'.")]
response = alfred.invoke({"messages": messages})

print("üé© R√©ponse d'Alfred :")
print(response['messages'][-1].content)
```

Sortie attendue :

```
üé© R√©ponse d'Alfred :
Lady Ada Lovelace est une math√©maticienne estim√©e et pionni√®re en informatique, souvent c√©l√©br√©e comme la premi√®re programmeuse informatique en raison de son travail sur la machine analytique de Charles Babbage.
```

Ce qui se passe dans cette √©tape finale :
- Nous initialisons un mod√®le Hugging Face en utilisant la classe `HuggingFaceEndpoint`. Nous g√©n√©rons aussi une interface de chat et ajoutons les outils.
- Nous cr√©ons notre agent (Alfred) comme un `StateGraph`, qui combine 2 n≈ìuds (`assistant`, `tools`) en utilisant une ar√™te
- Nous demandons √† Alfred de r√©cup√©rer des informations sur une invit√©e nomm√©e "Lady Ada Lovelace"

</hfoption>
</hfoptions>

## Exemple d'interaction

Pendant le gala, une conversation pourrait se d√©rouler comme ceci :

**Vous :** "Alfred, qui est ce monsieur qui parle √† l'ambassadeur ?"

**Alfred :** *recherche rapidement dans la base de donn√©es des invit√©s* "C'est le Dr. Nikola Tesla, monsieur. C'est un vieil ami de votre p√©riode √† l'universit√©. Il vient r√©cemment de breveter un nouveau syst√®me de transmission d'√©nergie sans fil et serait ravi d'en discuter avec vous. N'oubliez pas qu'il est passionn√© par les pigeons, donc cela pourrait faire une bonne conversation."

```json
{
    "name": "Dr. Nikola Tesla",
    "relation": "vieil ami des jours d'universit√©",  
    "description": "Le Dr. Nikola Tesla est un vieil ami de votre p√©riode √† l'universit√©. Il vient r√©cemment de breveter un nouveau syst√®me de transmission d'√©nergie sans fil et serait ravi d'en discuter avec vous. N'oubliez pas qu'il est passionn√© par les pigeons, donc cela pourrait faire une bonne conversation.",
    "email": "nikola.tesla@gmail.com"
}
```

## Aller plus loin

Maintenant qu'Alfred peut r√©cup√©rer des informations sur les invit√©s, consid√©rez comment vous pourriez am√©liorer ce syst√®me :

1. **Am√©liorer le r√©cup√©rateur** pour utiliser un algorithme plus sophistiqu√© comme ceux disponibles dans [`sentence-transformers`](https://www.sbert.net/)
2. **Impl√©menter une m√©moire de conversation** pour qu'Alfred se souvienne des interactions pr√©c√©dentes
3. **Combiner avec la recherche web** pour obtenir les derni√®res informations sur les invit√©s inconnus
4. **Int√©grer plusieurs index** pour obtenir des informations plus compl√®tes √† partir de sources v√©rifi√©es

Maintenant Alfred est enti√®rement √©quip√© pour g√©rer sans effort les questions sur les invit√©s, s'assurant que votre gala soit m√©moris√© comme l'√©v√©nement le plus sophistiqu√© et d√©licieux du si√®cle !

> [!TIP]
> Essayez d'√©tendre l'outil de r√©cup√©ration pour aussi retourner des amorces de conversation bas√©es sur les int√©r√™ts ou l'arri√®re-plan de chaque invit√©. Comment modifieriez-vous l'outil pour accomplir cela ?
>
> Quand vous avez termin√©, impl√©mentez votre outil de r√©cup√©ration d'invit√©s dans le fichier <code>retriever.py</code> du <i>Space</i>.
