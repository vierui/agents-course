# CrÃ©er des *workflows* agentiques dans LlamaIndex

Un *workflow* dans LlamaIndex fournit un moyen structurÃ© d'organiser votre code en Ã©tapes sÃ©quentielles et gÃ©rables.

Un tel *workflow* est crÃ©Ã© en dÃ©finissant des `Steps` qui sont dÃ©clenchÃ©s par des `Events`, et qui Ã©mettent eux-mÃªmes des `Events` pour dÃ©clencher d'autres Ã©tapes.
Jetons un coup d'Å“il Ã  Alfred montrant un *workflow* LlamaIndex pour une tÃ¢che de RAG.

![Workflow Schematic](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/workflows.png)

**Les *workflows* offrent plusieurs avantages clÃ©s :**

- Organisation claire du code en Ã©tapes discrÃ¨tes
- Architecture Ã©vÃ©nementielle pour un flux de contrÃ´le flexible
- Communication *type-safe* entre les Ã©tapes
- Gestion d'Ã©tat intÃ©grÃ©e
- Support pour des interactions d'agents simples et complexes

Comme vous l'avez peut-Ãªtre devinÃ©, **les *workflows* trouvent un excellent Ã©quilibre entre l'autonomie des agents tout en maintenant le contrÃ´le sur le *workflow* global.**

Alors, apprenons Ã  crÃ©er un *workflow* nous-mÃªmes !

## CrÃ©er des *Workflows*

> [!TIP]
> Vous pouvez suivre le code dans <a href="https://huggingface.co/agents-course/notebooks/blob/main/fr/unit2/llama-index/workflows.ipynb" target="_blank">ce <i>notebook</i></a> que vous pouvez exÃ©cuter avec Google Colab.

### CrÃ©ation de *Workflow* de base

<details>
<summary>Installer le package <i>Workflow</i></summary>
Comme introduit dans la <a href="./llama-hub">section sur le LlamaHub</a>, nous pouvons installer le package <code>Workflow</code> avec la commande suivante :

```python
pip install llama-index-utils-workflow
```
</details>

Nous pouvons crÃ©er un *workflow* en une seule Ã©tape en dÃ©finissant une classe qui hÃ©rite de `Workflow` et en dÃ©corant vos fonctions avec `@step`.
Nous devrons Ã©galement ajouter `StartEvent` et `StopEvent`, qui sont des Ã©vÃ©nements spÃ©ciaux utilisÃ©s pour indiquer le dÃ©but et la fin du *workflow*.

```python
from llama_index.core.workflow import StartEvent, StopEvent, Workflow, step

class MyWorkflow(Workflow):
    @step
    async def my_step(self, ev: StartEvent) -> StopEvent:
        # do something here
        return StopEvent(result="Hello, world!")


w = MyWorkflow(timeout=10, verbose=False)
result = await w.run()
```

Comme vous pouvez le voir, nous pouvons maintenant exÃ©cuter le *workflow* en appelant `w.run()`.

### Connecter plusieurs Ã©tapes

Pour connecter plusieurs Ã©tapes, nous **crÃ©ons des Ã©vÃ©nements personnalisÃ©s qui transportent des donnÃ©es entre les Ã©tapes.**
Pour ce faire, nous devons ajouter un `Event` qui est passÃ© entre les Ã©tapes et transfÃ¨re la sortie de la premiÃ¨re Ã©tape vers la deuxiÃ¨me Ã©tape.

```python
from llama_index.core.workflow import Event

class ProcessingEvent(Event):
    intermediate_result: str

class MultiStepWorkflow(Workflow):
    @step
    async def step_one(self, ev: StartEvent) -> ProcessingEvent:
        # Traitement des donnÃ©es initiales
        return ProcessingEvent(intermediate_result="Step 1 complete")

    @step
    async def step_two(self, ev: ProcessingEvent) -> StopEvent:
        # Utiliser le rÃ©sultat intermÃ©diaire
        final_result = f"Finished processing: {ev.intermediate_result}"
        return StopEvent(result=final_result)

w = MultiStepWorkflow(timeout=10, verbose=False)
result = await w.run()
result
```

L'indication de type est importante ici, car elle garantit que le *workflow* est exÃ©cutÃ© correctement. Compliquons un peu les choses !

### Boucles et branches

L'indication de type est la partie la plus puissante des *workflows* car elle nous permet de crÃ©er des branches, des boucles et des jointures pour faciliter des *workflows* plus complexes.

Montrons un exemple de **crÃ©ation d'une boucle** en utilisant l'opÃ©rateur union `|`.
Dans l'exemple ci-dessous, nous voyons que le `LoopEvent` est pris en entrÃ©e pour l'Ã©tape et peut Ã©galement Ãªtre retournÃ© en sortie.

```python
from llama_index.core.workflow import Event
import random


class ProcessingEvent(Event):
    intermediate_result: str


class LoopEvent(Event):
    loop_output: str


class MultiStepWorkflow(Workflow):
    @step
    async def step_one(self, ev: StartEvent | LoopEvent) -> ProcessingEvent | LoopEvent:
        if random.randint(0, 1) == 0:
            print("Bad thing happened")
            return LoopEvent(loop_output="Back to step one.")
        else:
            print("Good thing happened")
            return ProcessingEvent(intermediate_result="First step complete.")

    @step
    async def step_two(self, ev: ProcessingEvent) -> StopEvent:
        # Utiliser le rÃ©sultat intermÃ©diaire
        final_result = f"Finished processing: {ev.intermediate_result}"
        return StopEvent(result=final_result)


w = MultiStepWorkflow(verbose=False)
result = await w.run()
result
```

### Dessiner des *Workflows*

Nous pouvons Ã©galement dessiner des *workflows*. Utilisons la fonction `draw_all_possible_flows` pour dessiner le *workflow*. Cela stocke le *workflow* dans un fichier HTML.

```python
from llama_index.utils.workflow import draw_all_possible_flows

w = ... # tel que dÃ©fini dans la section prÃ©cÃ©dentetel que dÃ©fini dans la section prÃ©cÃ©dente
draw_all_possible_flows(w, "flow.html")
```

![workflow drawing](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/workflow-draw.png)

Il y a une derniÃ¨re astuce cool que nous couvrirons dans le cours, qui est la capacitÃ© d'ajouter de l'Ã©tat au *workflow*.

### Gestion d'Ã©tat

La gestion d'Ã©tat est utile quand vous voulez garder une trace de l'Ã©tat du *workflow*, pour que chaque Ã©tape ait accÃ¨s au mÃªme Ã©tat.
Nous pouvons faire cela en utilisant l'indication de type `Context` au-dessus d'un paramÃ¨tre dans la fonction d'Ã©tape.

```python
from llama_index.core.workflow import Context, StartEvent, StopEvent


@step
async def query(self, ctx: Context, ev: StartEvent) -> StopEvent:
    # stocker la requÃªte dans le contexte
    await ctx.store.set("query", "What is the capital of France?")

    # faire quelque chose avec le contexte et l'event
    val = ...

    # rÃ©cupÃ©rer la requÃªte dans le contexte
    query = await ctx.store.get("query")

    return StopEvent(result=val)
```

Parfait ! Maintenant vous savez comment crÃ©er des *workflows* de base dans LlamaIndex !

> [!TIP]
> Il y a quelques nuances plus complexes aux <i>workflows</i>, que vous pouvez apprendre dans <a href="https://docs.llamaindex.ai/en/stable/understanding/workflows/">la documentation LlamaIndex</a>.

Cependant, il y a une autre faÃ§on de crÃ©er des *workflows*, qui repose sur la classe `AgentWorkflow`. Jetons un coup d'Å“il Ã  comment nous pouvons utiliser cela pour crÃ©er un *workflow* multi-agents.

## Automatiser les *workflows* avec des *Multi-Agent Workflows*

Au lieu de la crÃ©ation manuelle de *workflows*, nous pouvons utiliser la **classe `AgentWorkflow` pour crÃ©er un *workflow* multi-agents**.
L'`AgentWorkflow` utilise des *Workflow Agents* pour vous permettre de crÃ©er un systÃ¨me d'un ou plusieurs agents qui peuvent collaborer et se passer des tÃ¢ches entre eux basÃ©es sur leurs capacitÃ©s spÃ©cialisÃ©es.
Cela permet de construire des systÃ¨mes d'agents complexes oÃ¹ diffÃ©rents agents gÃ¨rent diffÃ©rents aspects d'une tÃ¢che.
Au lieu d'importer des classes de `llama_index.core.agent`, nous importerons les classes d'agents de `llama_index.core.agent.workflow`.
Un agent doit Ãªtre dÃ©signÃ© comme l'agent racine dans le constructeur `AgentWorkflow`.
Quand un message utilisateur arrive, il est d'abord routÃ© vers l'agent racine.

Chaque agent peut ensuite :

- GÃ©rer la demande directement en utilisant leurs outils
- Passer le relais Ã  un autre agent mieux adaptÃ© Ã  la tÃ¢che
- Retourner une rÃ©ponse Ã  l'utilisateur

Voyons comment crÃ©er un *workflow* multi-agents.

```python
from llama_index.core.agent.workflow import AgentWorkflow, ReActAgent
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI

# DÃ©finir quelques outils
def add(a: int, b: int) -> int:
    """Additionner deux nombres."""
    return a + b

def multiply(a: int, b: int) -> int:
    """Multiplier deux nombres."""
    return a * b

llm = HuggingFaceInferenceAPI(model_name="Qwen/Qwen2.5-Coder-32B-Instruct")

# nous pouvons passer des fonctions directement sans FunctionTool -- les fn/docstring sont analysÃ©s pour le nom/description
multiply_agent = ReActAgent(
    name="multiply_agent",
    description="Is able to multiply two integers",
    system_prompt="A helpful assistant that can use a tool to multiply numbers.",
    tools=[multiply],
    llm=llm,
)

addition_agent = ReActAgent(
    name="add_agent",
    description="Is able to add two integers",
    system_prompt="A helpful assistant that can use a tool to add numbers.",
    tools=[add],
    llm=llm,
)

# CrÃ©er le workflow
workflow = AgentWorkflow(
    agents=[multiply_agent, addition_agent],
    root_agent="multiply_agent",
)

# ExÃ©cuter le systÃ¨me
response = await workflow.run(user_msg="Can you add 5 and 3?")
```

Les outils d'agents peuvent Ã©galement modifier l'Ã©tat du *workflow* que nous avons mentionnÃ© plus tÃ´t. Avant de commencer le *workflow*, nous pouvons fournir un dictionnaire d'Ã©tat initial qui sera disponible pour tous les agents.
L'Ã©tat est stockÃ© dans la clÃ© d'Ã©tat du contexte du *workflow*. Il sera injectÃ© dans le *state_prompt* qui augmente chaque nouveau message utilisateur.

Injectons un compteur pour compter les appels de fonctions en modifiant l'exemple prÃ©cÃ©dent :

```python
from llama_index.core.workflow import Context

# DÃ©finir quelques outils
async def add(ctx: Context, a: int, b: int) -> int:
    """Additionner deux nombres."""
    # mettre Ã  jour notre comptage
    cur_state = await ctx.store.get("state")
    cur_state["num_fn_calls"] += 1
    await ctx.store.set("state", cur_state)

    return a + b

async def multiply(ctx: Context, a: int, b: int) -> int:
    """Multiplier deux nombres."""
    # mettre Ã  jour notre comptage
    cur_state = await ctx.store.get("state")
    cur_state["num_fn_calls"] += 1
    await ctx.store.set("state", cur_state)

    return a * b

...

workflow = AgentWorkflow(
    agents=[multiply_agent, addition_agent],
    root_agent="multiply_agent",
    initial_state={"num_fn_calls": 0},
    state_prompt="Current state: {state}. User message: {msg}",
)

# exÃ©cuter le workflow avec le contexte
ctx = Context(workflow)
response = await workflow.run(user_msg="Can you add 5 and 3?", ctx=ctx)

# sortir et inspecter l'Ã©tat
state = await ctx.store.get("state")
print(state["num_fn_calls"])
```

FÃ©licitations ! Vous avez maintenant passer en revue les bases des agents dans LlamaIndex ! ðŸŽ‰

Continuons avec un dernier quiz pour solidifier vos connaissances ! ðŸš€