# Cr√©er des *workflows* agentiques dans LlamaIndex

Un *workflow* dans LlamaIndex fournit un moyen structur√© d'organiser votre code en √©tapes s√©quentielles et g√©rables.

Un tel *workflow* est cr√©√© en d√©finissant des `Steps` qui sont d√©clench√©s par des `Events`, et qui √©mettent eux-m√™mes des `Events` pour d√©clencher d'autres √©tapes.
Jetons un coup d'≈ìil √† Alfred montrant un *workflow* LlamaIndex pour une t√¢che *RAG*.

![Workflow Schematic](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/workflows.png)

**Les *workflows* offrent plusieurs avantages cl√©s :**

- Organisation claire du code en √©tapes discr√®tes
- Architecture √©v√©nementielle pour un flux de contr√¥le flexible
- Communication *type-safe* entre les √©tapes
- Gestion d'√©tat int√©gr√©e
- Support pour des interactions d'agents simples et complexes

Comme vous l'avez peut-√™tre devin√©, **les *workflows* trouvent un excellent √©quilibre entre l'autonomie des agents tout en maintenant le contr√¥le sur le *workflow* global.**

Alors, apprenons √† cr√©er un *workflow* nous-m√™mes !

## Cr√©er des *Workflows*

<Tip>
Vous pouvez suivre le code dans <a href="https://huggingface.co/agents-course/notebooks/blob/main/unit2/llama-index/workflows.ipynb" target="_blank">ce *notebook*</a> que vous pouvez ex√©cuter avec Google Colab.
</Tip>

### Cr√©ation de *Workflow* de base

<details>
<summary>Installer le package *Workflow*</summary>
Comme introduit dans la <a href="./llama-hub">section sur le *LlamaHub*</a>, nous pouvons installer le package *Workflow* avec la commande suivante :

```python
pip install llama-index-utils-workflow
```
</details>

Nous pouvons cr√©er un *workflow* en une seule √©tape en d√©finissant une classe qui h√©rite de `Workflow` et en d√©corant vos fonctions avec `@step`.
Nous devrons √©galement ajouter `StartEvent` et `StopEvent`, qui sont des √©v√©nements sp√©ciaux utilis√©s pour indiquer le d√©but et la fin du *workflow*.

```python
from llama_index.core.workflow import StartEvent, StopEvent, Workflow, step

class MyWorkflow(Workflow):
    @step
    async def my_step(self, ev: StartEvent) -> StopEvent:
        # do something here
        return StopEvent(result="Hello, world!")


w = MyWorkflow(timeout=10, verbose=False)
result = await w.run()
```

Comme vous pouvez le voir, nous pouvons maintenant ex√©cuter le *workflow* en appelant `w.run()`.

### Connecter plusieurs √©tapes

Pour connecter plusieurs √©tapes, nous **cr√©ons des √©v√©nements personnalis√©s qui transportent des donn√©es entre les √©tapes.**
Pour ce faire, nous devons ajouter un `Event` qui est pass√© entre les √©tapes et transf√®re la sortie de la premi√®re √©tape vers la deuxi√®me √©tape.

```python
from llama_index.core.workflow import Event

class ProcessingEvent(Event):
    intermediate_result: str

class MultiStepWorkflow(Workflow):
    @step
    async def step_one(self, ev: StartEvent) -> ProcessingEvent:
        # Process initial data
        return ProcessingEvent(intermediate_result="Step 1 complete")

    @step
    async def step_two(self, ev: ProcessingEvent) -> StopEvent:
        # Use the intermediate result
        final_result = f"Finished processing: {ev.intermediate_result}"
        return StopEvent(result=final_result)

w = MultiStepWorkflow(timeout=10, verbose=False)
result = await w.run()
result
```

L'indication de type est importante ici, car elle garantit que le *workflow* est ex√©cut√© correctement. Compliquons un peu les choses !

### Boucles et branches

L'indication de type est la partie la plus puissante des *workflows* car elle nous permet de cr√©er des branches, des boucles et des jointures pour faciliter des *workflows* plus complexes.

Montrons un exemple de **cr√©ation d'une boucle** en utilisant l'op√©rateur union `|`.
Dans l'exemple ci-dessous, nous voyons que le `LoopEvent` est pris en entr√©e pour l'√©tape et peut √©galement √™tre retourn√© en sortie.

```python
from llama_index.core.workflow import Event
import random


class ProcessingEvent(Event):
    intermediate_result: str


class LoopEvent(Event):
    loop_output: str


class MultiStepWorkflow(Workflow):
    @step
    async def step_one(self, ev: StartEvent | LoopEvent) -> ProcessingEvent | LoopEvent:
        if random.randint(0, 1) == 0:
            print("Bad thing happened")
            return LoopEvent(loop_output="Back to step one.")
        else:
            print("Good thing happened")
            return ProcessingEvent(intermediate_result="First step complete.")

    @step
    async def step_two(self, ev: ProcessingEvent) -> StopEvent:
        # Use the intermediate result
        final_result = f"Finished processing: {ev.intermediate_result}"
        return StopEvent(result=final_result)


w = MultiStepWorkflow(verbose=False)
result = await w.run()
result
```

### Dessiner des *Workflows*

Nous pouvons √©galement dessiner des *workflows*. Utilisons la fonction `draw_all_possible_flows` pour dessiner le *workflow*. Cela stocke le *workflow* dans un fichier *HTML*.

```python
from llama_index.utils.workflow import draw_all_possible_flows

w = ... # as defined in the previous section
draw_all_possible_flows(w, "flow.html")
```

![workflow drawing](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/workflow-draw.png)

Il y a une derni√®re astuce cool que nous couvrirons dans le cours, qui est la capacit√© d'ajouter de l'√©tat au *workflow*.

### Gestion d'√©tat

La gestion d'√©tat est utile quand vous voulez garder une trace de l'√©tat du *workflow*, pour que chaque √©tape ait acc√®s au m√™me √©tat.
Nous pouvons faire cela en utilisant l'indication de type `Context` au-dessus d'un param√®tre dans la fonction d'√©tape.

```python
from llama_index.core.workflow import Context, StartEvent, StopEvent


@step
async def query(self, ctx: Context, ev: StartEvent) -> StopEvent:
    # store query in the context
    await ctx.store.set("query", "What is the capital of France?")

    # do something with context and event
    val = ...

    # retrieve query from the context
    query = await ctx.store.get("query")

    return StopEvent(result=val)
```

Parfait ! Maintenant vous savez comment cr√©er des *workflows* de base dans LlamaIndex !

<Tip>Il y a quelques nuances plus complexes aux *workflows*, que vous pouvez apprendre dans <a href="https://docs.llamaindex.ai/en/stable/understanding/workflows/">la documentation LlamaIndex</a>.</Tip>

Cependant, il y a une autre fa√ßon de cr√©er des *workflows*, qui repose sur la classe `AgentWorkflow`. Jetons un coup d'≈ìil √† comment nous pouvons utiliser cela pour cr√©er un *workflow* multi-agents.

## Automatiser les *workflows* avec des *Multi-Agent Workflows*

Au lieu de la cr√©ation manuelle de *workflows*, nous pouvons utiliser la **classe `AgentWorkflow` pour cr√©er un *workflow* multi-agents**.
L'`AgentWorkflow` utilise des *Workflow Agents* pour vous permettre de cr√©er un syst√®me d'un ou plusieurs agents qui peuvent collaborer et se passer des t√¢ches entre eux bas√©es sur leurs capacit√©s sp√©cialis√©es.
Cela permet de construire des syst√®mes d'agents complexes o√π diff√©rents agents g√®rent diff√©rents aspects d'une t√¢che.
Au lieu d'importer des classes de `llama_index.core.agent`, nous importerons les classes d'agents de `llama_index.core.agent.workflow`.
Un agent doit √™tre d√©sign√© comme l'agent racine dans le constructeur `AgentWorkflow`.
Quand un message utilisateur arrive, il est d'abord rout√© vers l'agent racine.

Chaque agent peut ensuite :

- G√©rer la demande directement en utilisant leurs *tools*
- Passer le relais √† un autre agent mieux adapt√© √† la t√¢che
- Retourner une r√©ponse √† l'utilisateur

Voyons comment cr√©er un *workflow* multi-agents.

```python
from llama_index.core.agent.workflow import AgentWorkflow, ReActAgent
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI

# Define some tools
def add(a: int, b: int) -> int:
    """Add two numbers."""
    return a + b

def multiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b

llm = HuggingFaceInferenceAPI(model_name="Qwen/Qwen2.5-Coder-32B-Instruct")

# we can pass functions directly without FunctionTool -- the fn/docstring are parsed for the name/description
multiply_agent = ReActAgent(
    name="multiply_agent",
    description="Is able to multiply two integers",
    system_prompt="A helpful assistant that can use a tool to multiply numbers.",
    tools=[multiply],
    llm=llm,
)

addition_agent = ReActAgent(
    name="add_agent",
    description="Is able to add two integers",
    system_prompt="A helpful assistant that can use a tool to add numbers.",
    tools=[add],
    llm=llm,
)

# Create the workflow
workflow = AgentWorkflow(
    agents=[multiply_agent, addition_agent],
    root_agent="multiply_agent",
)

# Run the system
response = await workflow.run(user_msg="Can you add 5 and 3?")
```

Les *tools* d'agents peuvent √©galement modifier l'√©tat du *workflow* que nous avons mentionn√© plus t√¥t. Avant de commencer le *workflow*, nous pouvons fournir un dictionnaire d'√©tat initial qui sera disponible pour tous les agents.
L'√©tat est stock√© dans la cl√© d'√©tat du contexte du *workflow*. Il sera inject√© dans le *state_prompt* qui augmente chaque nouveau message utilisateur.

Injectons un compteur pour compter les appels de fonctions en modifiant l'exemple pr√©c√©dent :

```python
from llama_index.core.workflow import Context

# Define some tools
async def add(ctx: Context, a: int, b: int) -> int:
    """Add two numbers."""
    # update our count
    cur_state = await ctx.store.get("state")
    cur_state["num_fn_calls"] += 1
    await ctx.store.set("state", cur_state)

    return a + b

async def multiply(ctx: Context, a: int, b: int) -> int:
    """Multiply two numbers."""
    # update our count
    cur_state = await ctx.store.get("state")
    cur_state["num_fn_calls"] += 1
    await ctx.store.set("state", cur_state)

    return a * b

...

workflow = AgentWorkflow(
    agents=[multiply_agent, addition_agent],
    root_agent="multiply_agent",
    initial_state={"num_fn_calls": 0},
    state_prompt="Current state: {state}. User message: {msg}",
)

# run the workflow with context
ctx = Context(workflow)
response = await workflow.run(user_msg="Can you add 5 and 3?", ctx=ctx)

# pull out and inspect the state
state = await ctx.store.get("state")
print(state["num_fn_calls"])
```

F√©licitations ! Vous avez maintenant ma√Ætris√© les bases des *Agents* dans LlamaIndex ! üéâ

Continuons avec un dernier quiz pour solidifier vos connaissances ! üöÄ